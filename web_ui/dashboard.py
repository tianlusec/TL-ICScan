import streamlit as st
import sqlite3
import pandas as pd
import json
from datetime import datetime

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Tianlu æƒ…æŠ¥çœ‹æ¿",
    page_icon="ğŸ›¡ï¸",
    layout="wide"
)

st.title("ğŸ›¡ï¸ Tianlu æ¼æ´æƒ…æŠ¥æ”¶é›†ç³»ç»Ÿ")

# ä¾§è¾¹æ ï¼šç­›é€‰æ¡ä»¶
st.sidebar.header("ğŸ” ç­›é€‰æ¡ä»¶")

# æœç´¢æ¡†
search_term = st.sidebar.text_input("å…³é”®å­—æœç´¢ (æ ‡é¢˜/æè¿°)", "")

# ä¸¥é‡ç­‰çº§ç­›é€‰
severity_options = ["CRITICAL", "HIGH", "MEDIUM", "LOW", "UNKNOWN"]
selected_severity = st.sidebar.multiselect("ä¸¥é‡ç­‰çº§", severity_options, default=["CRITICAL", "HIGH"])

# æ—¥æœŸèŒƒå›´
# é»˜è®¤å±•ç¤ºæœ€è¿‘ 30 å¤©
default_start = datetime.now().date().replace(day=1)
date_range = st.sidebar.date_input("å‘å¸ƒæ—¶é—´èŒƒå›´", [])

# è¿æ¥æ•°æ®åº“
@st.cache_data(ttl=60)  # ç¼“å­˜æ•°æ® 60 ç§’
def load_data(severity_list, search_text, date_filter):
    db_path = "tianlu_intel_v2.db"
    import os
    
    # Try to find DB in current or parent directory
    if not os.path.exists(db_path):
        if os.path.exists("../tianlu_intel_v2.db"):
            db_path = "../tianlu_intel_v2.db"
        elif os.path.exists("tianlu_intel.db"):
            db_path = "tianlu_intel.db"
        elif os.path.exists("../tianlu_intel.db"):
            db_path = "../tianlu_intel.db"

    conn = sqlite3.connect(db_path)
    
    # Check if is_in_kev column exists (for backward compatibility)
    cursor = conn.cursor()
    cursor.execute("PRAGMA table_info(cve_records)")
    columns = [info[1] for info in cursor.fetchall()]
    has_kev = "is_in_kev" in columns
    
    select_cols = "cve_id, severity, cvss_v3_score, title, publish_date, vendors, products, sources"
    if has_kev:
        select_cols += ", is_in_kev, attack_vector"

    query = f"SELECT {select_cols} FROM cve_records WHERE 1=1"
    params = []

    # ä¸¥é‡ç­‰çº§è¿‡æ»¤
    if severity_list:
        placeholders = ",".join("?" * len(severity_list))
        query += f" AND severity IN ({placeholders})"
        params.extend(severity_list)
    
    # å…³é”®å­—è¿‡æ»¤
    if search_text:
        query += " AND (title LIKE ? OR description LIKE ? OR cve_id LIKE ?)"
        wildcard = f"%{search_text}%"
        params.extend([wildcard, wildcard, wildcard])

    # æ—¥æœŸè¿‡æ»¤ (ç®€å•å¤„ç†ï¼Œå‡è®¾ date_filter æ˜¯ä¸ªåˆ—è¡¨)
    if len(date_filter) == 2:
        start_date, end_date = date_filter
        query += " AND publish_date >= ? AND publish_date <= ?"
        params.extend([start_date.isoformat(), end_date.isoformat()])

    query += " ORDER BY publish_date DESC LIMIT 500"
    
    try:
        df = pd.read_sql_query(query, conn, params=params)
    finally:
        conn.close()
    
    return df

# åŠ è½½æ•°æ®
df = load_data(selected_severity, search_term, date_range)

# å±•ç¤ºç»Ÿè®¡ä¿¡æ¯
col1, col2, col3 = st.columns(3)
col1.metric("å½“å‰å±•ç¤ºæ•°é‡", len(df))
if not df.empty:
    col2.metric("æœ€é«˜ CVSS åˆ†æ•°", df["cvss_v3_score"].max())
    latest_date = pd.to_datetime(df["publish_date"]).max()
    col3.metric("æœ€æ–°æ”¶å½•", latest_date.strftime("%Y-%m-%d") if pd.notnull(latest_date) else "-")

# ä¸»è¡¨æ ¼å±•ç¤º
st.subheader("ğŸ“‹ æ¼æ´åˆ—è¡¨")

if not df.empty:
    # æ ¼å¼åŒ–ä¸€ä¸‹æ•°æ®ï¼Œè®© JSON å­—æ®µå¥½çœ‹ç‚¹
    # ä½† Streamlit çš„ dataframe äº¤äº’æ€§å·²ç»ä¸é”™äº†ï¼Œç›´æ¥å±•ç¤º
    
    # è‡ªå®šä¹‰åˆ—é…ç½®
    column_config = {
        "cve_id": "CVE ID",
        "severity": st.column_config.TextColumn("ä¸¥é‡ç­‰çº§", help="Low, Medium, High, Critical"),
        "cvss_v3_score": st.column_config.NumberColumn("CVSS v3", format="%.1f"),
        "title": "æ ‡é¢˜",
        "publish_date": st.column_config.DatetimeColumn("å‘å¸ƒæ—¶é—´", format="YYYY-MM-DD HH:mm"),
        "vendors": "å‚å•†",
        "products": "äº§å“",
        "sources": "æ¥æº",
    }
    
    if "is_in_kev" in df.columns:
        column_config["is_in_kev"] = st.column_config.CheckboxColumn("KEV?", help="æ˜¯å¦åœ¨ CISA KEV åˆ—è¡¨ä¸­")
    if "attack_vector" in df.columns:
        column_config["attack_vector"] = "æ”»å‡»å‘é‡"

    st.dataframe(
        df,
        column_config=column_config,
        use_container_width=True,
        hide_index=True,
        height=600
    )
else:
    st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¼æ´æƒ…æŠ¥ã€‚è¯·å°è¯•è°ƒæ•´å·¦ä¾§çš„ç­›é€‰æ¡ä»¶ã€‚")

# è¯¦æƒ…æŸ¥çœ‹å™¨ (ç®€å•ç‰ˆ)
st.divider()
st.subheader("ğŸ“ å¿«é€Ÿè¯¦æƒ…æŸ¥çœ‹")
cve_to_check = st.text_input("è¾“å…¥ CVE ID æŸ¥çœ‹å®Œæ•´è¯¦æƒ… (ä¾‹å¦‚ CVE-2025-13576)", "")

if cve_to_check:
    conn = sqlite3.connect("tianlu_intel.db")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM cve_records WHERE cve_id = ?", (cve_to_check,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        # è·å–åˆ—å
        col_names = [description[0] for description in cursor.description]
        data = dict(zip(col_names, row))
        
        st.markdown(f"### {data['cve_id']}")
        st.markdown(f"**æ ‡é¢˜**: {data['title']}")
        
        c1, c2, c3 = st.columns(3)
        c1.markdown(f"**ä¸¥é‡ç­‰çº§**: {data['severity']}")
        c2.markdown(f"**CVSS v3**: {data['cvss_v3_score']}")
        c3.markdown(f"**å‘å¸ƒæ—¶é—´**: {data['publish_date']}")
        
        st.markdown("#### æè¿°")
        st.info(data['description'])
        
        st.markdown("#### å½±å“èŒƒå›´")
        st.json({
            "Vendors": json.loads(data['vendors']),
            "Products": json.loads(data['products'])
        })
        
        st.markdown("#### å‚è€ƒé“¾æ¥")
        for ref in json.loads(data['references']):
            st.markdown(f"- {ref}")
            
    else:
        st.error("æœªæ‰¾åˆ°è¯¥ CVE IDã€‚")
